{
  "basics": {
    "name": "Prashil Bhimani",
    "label": ["Big Data Engineer", "Platform Engineer", "Designer", "Full Stack Developer", "Software Engineer"],
    "picture": "images/profile.png",
    "x_title": "Hello, Nice to see you here!",
    "summary": "I am software engineer interested in building real time data pipeline and big data analytics software. Currently I am working at @Twitter building realtime Data Pipelines and analytics platform for the Public APIs. In my leisure time I love watching Netflix documentaries, History videos, and keep a track of Cricket games ",
    "location": {
      "country": "US",
      "countryCode": "US",
      "region": "Bay Area"
    },
    "profiles": [
      {
        "network": "Twitter",
        "username": "_prashil_",
        "url": "https://twitter.com/_prashil_",
        "x_icon":"fab fa-2x fa-twitter"
      },
      {
        "network": "LinkedIn",
        "username": "prashil-bhimani",
        "url": "https://www.linkedin.com/in/prashil-bhimani/",
        "x_icon": "fab fa-2x fa-linkedin"
      },
      {
        "network": "GitHub",
        "username": "prashilbhimani",
        "url": "https://github.com/prashilbhimani",
        "x_icon": "fab fa-2x fa-github"
      },
      {
        "network": "Mail",
        "username": "prashilbhimani24",
        "url": "mailto:prashilbhimani24@gmail.com",
        "x_icon": "fa fa-2x fa-inbox"
      }
    ]
  },
  "work": [
    {
      "company": "Twitter",
      "position": "Software Engineer II",
      "website": "www.twitter.com",
      "startDate": "2020-09-14",
      "summary": "Building a managed real time streaming compute platform at Twitter to handle hybrid cloud real time data compute pipeline with stateful / stateless computations"
    },
    {
      "company": "Twitter",
      "position": "Software Engineer I",
      "website": "www.twitter.com",
      "startDate": "2019-07-22",
      "endDate": "2020-09-14",
      "summary": "Working for the Twitter Public API platform (DES) and built a pipeline to track and analyze usage of the APIs"
    },
    {
      "company": "University of Colorado, Boulder",
      "position": "Lead Teaching Assistant",
      "website": "https://www.colorado.edu/cs/",
      "startDate": "2018-07-15",
      "endDate": "2019-05-08",
      "summary": "Lead a team of over 70 Teaching Assistants and taugh Data Structures and Programming for 3 semesters at the CS department in CU Boulder",
      "highlights": []
    },
    {
      "company": "Goldman Sachs",
      "position": "Analyts",
      "website": "https://www.goldmansachs.com/",
      "startDate": "2018-05-31",
      "endDate": "2018-08-08",
      "summary": "Joined Goldman Sachs as an Analyst to work on the Controller's Tech division and built streamlined approval process for quaterly financial statements for funds"
    },
    {
      "company": "University of Colorado, Boulder",
      "position": "Teaching Assistant",
      "website": "https://www.colorado.edu/cs/",
      "startDate": "2018-01-15",
      "endDate": "2019-05-08",
      "summary": "Taugh Data Structures and Programming for 3 semesters at the CS department in CU Boulder to over 200 students. Awarded 'Exceptional Teaching Assistant Award' for 2 semesters",
      "highlights": []
    } 
  ],
  "education": [
    {
      "institution": "University of Colorado, Boulder",
      "area": "Computer Scince",
      "studyType": "Masters",
      "startDate": "2017-08-17",
      "endDate": "2019-05-08",
      "courses": [
        "Object Oriented Programming",
        "Database Management Systems",
        "Operating systems and computers architectures",
        "Requirements Engineering"
      ]
    },
    {
      "institution": "Veermata Jijabai Technological Institute (VJTI)",
      "area": "Information Technology",
      "studyType": "Bachelor",
      "startDate": "2013-06-13",
      "endDate": "2017-05-01",
      "courses": [
        "Object Oriented Programming",
        "Database Management Systems",
        "Operating systems and computers architectures",
        "Requirements Engineering"
      ]
    }
  ],
  "skills": [
    {
      "name": "GCP",
      "level": "Proficient",
      "category": "Cloud & Distributed Systems"
    },
    {
      "name": "Kubernetes",
      "level": "Beginner",
      "category": "Cloud & Distributed Systems"
    },
    {
      "name": "Kafka/ Messsaging System",
      "level": "Proficient",
      "category": "Cloud & Distributed Systems"
    },
    {
      "name": "Map Reduce",
      "level": "Proficient",
      "category": "Cloud & Distributed Systems"
    },
    {
      "name": "Elastic Search",
      "level": "Competent",
      "category": "Cloud & Distributed Systems"
    },
    {
      "name": "Apache Beam",
      "level": "Proficient",
      "category": "Cloud & Distributed Systems"
    },
    {
      "name": "Dataflow",
      "level": "Proficient",
      "category": "Cloud & Distributed Systems"
    },
    


    {
      "name": "Full Stack Development",
      "level": "Proficient",
      "category": "Software Development"
    },
    {
      "name": "Front End Development",
      "level": "Competent",
      "category": "Software Development"
    },
        {
      "name": "REST API",
      "level": "Expert",
      "category": "Software Development"
    },
    {
      "name": "Object Oriented Programming",
      "level": "Expert",
      "category": "Software Development"
    },
    {
      "name": "Design Patterns",
      "level": "Competent",
      "category": "Software Development"
    },
    {
      "name": "Git",
      "level": "Proficient",
      "category": "Software Development"
    },

    {
      "name": "Java",
      "level": "Expert",
      "category": "Languages"
    },
    {
      "name": "Python",
      "level": "Proficient",
      "category": "Languages"
    },
    {
      "name": "Scala",
      "level": "Competent",
      "category": "Languages"
    },
    {
      "name": "C++",
      "level": "Competent",
      "category": "Languages"
    },
    {
      "name": "C",
      "level": "Beginner",
      "category": "Languages"
    },
    {
      "name": "JavaScript",
      "level": "Competent",
      "category": "Languages"
    },
    {
      "name": "SQL",
      "level": "Proficient",
      "category": "Languages"
    }
  ],
  "languages": [
    {
      "language": "English",
      "fluency": "Fluent speaker"
    },
    {
      "language": "Hindi",
      "fluency": "Fluent speaker"
    },
    {
      "language": "Gujrati",
      "fluency": "Native speaker"
    },
    {
      "language": "Marathi",
      "fluency": "Fluent speaker"
    }
  ],
  "interests": [
    {
      "name": "Data Engineer",
      "x_icon": "fa-database"
    },
    {
      "name": "Data Analyst",
      "x_icon": "fa-cloud"
    },
    {
      "name": "Software Engineer",
      "x_icon": "fa-code"
    },
    {
      "name": "Full Stack Developer",
      "x_icon": "fa-window-maximize"
    }
  ],
  "projects": [
    {
      "name": "Project EPIC",
      "one_line": "Build a platform to crowd source information about crisis and natural disasters to aid emergency response.",
      "description": [
        "TODO",
        "TODO",
        "TODO"
      ],
      "technology": ["Hadoop", "Apache Storm", "Apache Spark", "Neo4J", "GCP"],
      "start_date": "Aug 2018",
      "link": "https://epic.cs.colorado.edu/",
      "link_x_icon": "fa-github"
    },
    {
      "name": " Plug N Search",
      "one_line": "A managed service to scalably upload huge datasets and get Search as a service",
      "description": [
        "The goal of the project was to give search infrastructure as a service where customers can upload their datasets and plug in a search index in their application backed by their data.",
        "The search infrastruture was based on elastic search where customers can define their fields to index and have fine grained control over their data without the over head of maintaining elastic search clusters.",
        "To support uploads of large data we supported peer to peer network uploads using torrent protocol.",
        "Along with basic search the service would provide precomputed analytics on several basic fields."
      ],
      "technology": ["Elastic Search", "AWS", "BitTorrent", "Apache Spark"],
      "start_date": "Sept 2018",
      "link": "",
      "link_x_icon": "fa-github"
    },
    {
      "name": "Echo chambers @ Twitter",
      "one_line": "Reasearching how echo chambers form in Twitter and ways of detection.",
      "description": [
        "Social Networks exhibit graph like communities and goal of the research was to apply graph algorithms to detect sub communities in a particular community.",
        "We used Twitter Data for on the community of users who talk about climate change on the social network.",
        "The study processed over a milllion tweets with 100k+ users and found echo chambers within the community where people using the Louvain algorithm.",
        "We uncovered that Twitter communities have echo chambers where the interactions between communities supporting climate change and considering it a hoax is minimal."
      ],
      "technology": ["Hadoop", "Apache Storm", "Apache Spark", "Neo4J", "GCP"],
      "start_date": "Aug 2018",
      "link": "",
      "link_x_icon": "fa-github"
    },
    {
      "name": "Connect 4 Game",
      "one_line": "Play the classic game of Connect 4 against a computer or another player",
      "description": [
        "We developed a Connect 4 player using Q learning where the Q-Table was mocked by a structured neural network.",
        "This lead to reducing in training time over an unsupervised network.",
        "The player was trained on a 3 layerd structured neural network to determine Q values.",
        "We used opposing players which played based on different strateries like Limited depth Minimax, Monte Carlo, and Random moves to train our model."
      ],
      "technology": ["Python", "Keras", "TensorFlow"],
      "start_date": "Jan 2018",
      "link": "",
      "link_x_icon": "fa-github"
    },
    {
      "name": "Tweet-a-lyzer",
      "one_line": "Analyze how people are talking about topics in real time on Twitter based in different places in the past week",
      "description": [
        "Brands are very keen to understand public response to their products across various regions.",
        "We used Twitter's live stream API to get live tweets about people tweeting about a certain product and what are their sentiments about it.",
        "The system was backed by real time processing of the tweets and comuting the overall sentiments by regions for the given keyword."
      ],
      "technology": ["Kafka", "Apache Storm", "Mongo DB", "AWS", "Mapbox", "D3"],
      "start_date": "Jan 2018",
      "link": "",
      "link_x_icon": "fa-github"
    },
    {
      "name": "Fixed Shared-Cab Spots",
      "one_line": "Find optimal spots in NYC to have cab stops so people can share a cab between most frequented routes.",
      "description": [
        "Using the public dataset of NYC cabs we tried to figure which would be the best spots for share-a-cab booths",
        "We minied for frequent pairs based on location, time and minimum need to walk we proposed pairs of pickup and drop locations where it would be economical to run such a service"
      ],
      "technology": ["Python", "TIGER dataset", "PostgreSQL"],
      "start_date": "Aug 2017",
      "link": "",
      "link_x_icon": "fa-github"
    },
    {
      "name": "University Records System",
      "one_line": "Built a web app for VJTI to handle records of curricular/extra curricular activities for 10k+ students",
      "description": [
        "TODO",
        "TODO"
      ],
      "technology": ["Python", "MinMax Algorithm", "GCP"],
      "start_date": "Aug 2016",
      "link": "",
      "link_x_icon": "fa-github"
    },
    {
      "name": "Reviews based results",
      "one_line": "Built models that analyze reviews for a business to see things they are good/bad at and rank search results for users based on what they value rather than overall reviews",
      "description": [
        "We modeled a pandemic as a game where the goal was to reduce the infection spread within a network.",
        "For each \"move\" the infection can infect 1 person and the player could immunize 1 person",
        "We modeled a min max algorithm to detect the best possible case where least people are infected before the infection can be contained."
      ],
      "technology": ["Python", "MinMax Algorithm", "GCP"],
      "start_date": "Aug 2017",
      "link": "",
      "link_x_icon": "fa-github"
    }
  ]
}
